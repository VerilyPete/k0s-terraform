name: 'Terraform Apply'

on:
  push:
    branches:
      - main
    paths:
      - 'terraform/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      destroy:
        description: 'Destroy infrastructure instead of apply'
        required: false
        default: false
        type: boolean
      force_rebuild:
        description: 'Force complete cluster rebuild (taint all instances)'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  id-token: write

env:
  TF_VERSION: '1.12.1'
  OCI_CLI_USER: ${{ secrets.OCI_CLI_USER }}
  OCI_CLI_TENANCY: ${{ secrets.OCI_CLI_TENANCY }}
  OCI_CLI_FINGERPRINT: ${{ secrets.OCI_CLI_FINGERPRINT }}
  OCI_CLI_KEY_CONTENT: ${{ secrets.OCI_CLI_KEY_CONTENT }}
  OCI_CLI_REGION: ${{ secrets.OCI_CLI_REGION }}
  # These environment variables are no longer needed - using terraform.tfvars instead

jobs:
  terraform-apply:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      controller_ip: ${{ steps.outputs.outputs.controller_ip }}
      controller_hostname: ${{ steps.outputs.outputs.controller_hostname }}
      environment: ${{ steps.outputs.outputs.environment }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Setup OCI CLI
        run: |
          # Install OCI CLI
          curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh > install.sh
          chmod +x install.sh
          ./install.sh --accept-all-defaults
          rm install.sh
          
          # Configure OCI CLI
          mkdir -p /home/runner/.oci
          echo "${{ secrets.OCI_CLI_KEY_CONTENT }}" > /home/runner/.oci/oci_api_key.pem
          chmod 600 /home/runner/.oci/oci_api_key.pem
          
          cat > /home/runner/.oci/config << EOF
          [DEFAULT]
          user=${{ secrets.OCI_CLI_USER }}
          fingerprint=${{ secrets.OCI_CLI_FINGERPRINT }}
          tenancy=${{ secrets.OCI_CLI_TENANCY }}
          region=${{ secrets.OCI_CLI_REGION }}
          key_file=/home/runner/.oci/oci_api_key.pem
          EOF
          
          chmod 600 /home/runner/.oci/config
          
          # Debug: Check if files were created
          echo "=== Debug: OCI Configuration ==="
          ls -la /home/runner/.oci/
          echo "Private key file exists:" 
          test -f /home/runner/.oci/oci_api_key.pem && echo "YES" || echo "NO"
          echo "Private key file size:" 
          wc -c /home/runner/.oci/oci_api_key.pem 2>/dev/null || echo "Cannot read file"
          echo "Private key file permissions:" 
          ls -la /home/runner/.oci/oci_api_key.pem
          echo "Private key format check:"
          head -1 /home/runner/.oci/oci_api_key.pem 2>/dev/null || echo "Cannot read private key"
          echo "Private key validation:"
          openssl rsa -in /home/runner/.oci/oci_api_key.pem -check -noout 2>/dev/null && echo "‚úÖ Private key is valid RSA format" || echo "‚ùå Private key validation failed"
          echo "Current working directory: $(pwd)"
          echo "Home directory: $HOME"

      - name: Determine environment
        id: env
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            echo "environment=staging" >> $GITHUB_OUTPUT
          fi

      - name: Create terraform.tfvars
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: |
          # Create terraform.tfvars with private key path
          cat > terraform.tfvars << EOF
          # OCI Provider Configuration
          tenancy_ocid       = "${{ secrets.OCI_CLI_TENANCY }}"
          user_ocid          = "${{ secrets.OCI_CLI_USER }}"
          fingerprint        = "${{ secrets.OCI_CLI_FINGERPRINT }}"
          private_key_path   = "/home/runner/.oci/oci_api_key.pem"
          region             = "${{ secrets.OCI_CLI_REGION }}"
          oci_namespace      = "${{ secrets.OCI_NAMESPACE }}"
          EOF
          
          # Add infrastructure configuration
          cat >> terraform.tfvars << EOF
          # Infrastructure Configuration
          compartment_id      = "${{ secrets.OCI_COMPARTMENT_ID }}"
          availability_domain = "${{ secrets.OCI_AVAILABILITY_DOMAIN }}"
          subnet_id          = "${{ secrets.OCI_PRIVATE_SUBNET }}"
          vcn_id             = "${{ secrets.OCI_VCN_ID }}"
          route_table_id     = "${{ steps.env.outputs.environment == 'staging' && secrets.OCI_ROUTE_TABLE_STAGING || secrets.OCI_ROUTE_TABLE_PRODUCTION }}"
          image_id           = "${{ secrets.OCI_CUSTOM_IMAGE }}"
          ssh_public_key     = "${{ secrets.SSH_PUBLIC_KEY }}"
          tailscale_auth_key = "${{ secrets.TAILSCALE_AUTH_KEY }}"
          EOF
          
          # ssh_private_key removed - no longer using provisioners
          
          # No path expansion needed - using absolute paths
          
          # Debug: Show generated terraform.tfvars
          echo "=== Debug: Generated terraform.tfvars ==="
          cat terraform.tfvars

      - name: Verify OCI Configuration
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: |
          echo "=== Verifying OCI Configuration Before Terraform ==="
          
          # Check private key file exists
          PRIVATE_KEY_PATH=$(grep private_key_path terraform.tfvars | cut -d'"' -f2)
          echo "Private key path from terraform.tfvars: $PRIVATE_KEY_PATH"
          
          if [ -f "$PRIVATE_KEY_PATH" ]; then
            echo "‚úÖ Private key file exists at: $PRIVATE_KEY_PATH"
            ls -la "$PRIVATE_KEY_PATH"
            
            # Verify the private key file is not empty and has correct format
            if [ -s "$PRIVATE_KEY_PATH" ]; then
              echo "‚úÖ Private key file is not empty"
              # Check if it starts with a valid private key header
              if head -1 "$PRIVATE_KEY_PATH" | grep -q "BEGIN.*PRIVATE KEY"; then
                echo "‚úÖ Private key file has valid format"
              else
                echo "‚ö†Ô∏è  Private key file format may be incorrect (should start with -----BEGIN...PRIVATE KEY-----)"
              fi
            else
              echo "‚ùå Private key file is empty"
              exit 1
            fi
          else
            echo "‚ùå Private key file NOT found at: $PRIVATE_KEY_PATH"
            echo "Checking absolute path directory:"
            ls -la /home/runner/.oci/ || echo "/home/runner/.oci directory not found"
            exit 1
          fi

      - name: OCI Backend Configuration Info
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: |
          echo "=== OCI Backend Configuration ==="
          echo "Using OCI native backend for state management"
          echo "Bucket: terraform-state-${{ steps.env.outputs.environment }}"
          echo "Namespace: ${{ secrets.OCI_NAMESPACE }}"
          echo "Region: ${{ secrets.OCI_CLI_REGION }}"
          echo "Key: k0s-cluster/terraform.tfstate"
          echo "Auth: APIKey (using ~/.oci/config profile DEFAULT)"

      - name: Terraform Init
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: |
          terraform init \
            -backend-config="namespace=${{ secrets.OCI_NAMESPACE }}" \
            -backend-config="region=${{ secrets.OCI_CLI_REGION }}"

      - name: Check for tainted instances and cluster state
        if: github.event.inputs.destroy != 'true'
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        id: taint_check
        run: |
          echo "üîç Checking terraform state for tainted instances..."
          
          # Check for forced rebuild option
          if [ "${{ github.event.inputs.force_rebuild }}" == "true" ]; then
            echo "üîÑ Force rebuild requested by user"
            echo "tainted=true" >> $GITHUB_OUTPUT
          else
            # Check for tainted resources
            TAINTED_RESOURCES=$(terraform state list | xargs -I {} terraform state show {} | grep -l "tainted = true" || true)
            
            if [ -n "$TAINTED_RESOURCES" ]; then
              echo "‚ö†Ô∏è  Found tainted resources:"
              echo "$TAINTED_RESOURCES"
              echo "tainted=true" >> $GITHUB_OUTPUT
            else
              echo "‚úÖ No tainted resources found"
              echo "tainted=false" >> $GITHUB_OUTPUT
            fi
          fi
          
          # Check current state for any instances
          echo ""
          echo "üìä Current infrastructure state:"
          EXISTING_INSTANCES=$(terraform state list | grep -E "(controller|worker)" || true)
          if [ -n "$EXISTING_INSTANCES" ]; then
            echo "$EXISTING_INSTANCES"
            INFRASTRUCTURE_EXISTS=true
          else
            echo "No instances found"
            INFRASTRUCTURE_EXISTS=false
          fi
          
          # Get plan to see what changes are needed
          echo ""
          echo "üìã Checking terraform plan..."
          terraform plan -detailed-exitcode -no-color > plan_output.txt 2>&1
          PLAN_EXIT_CODE=$?
          
          case $PLAN_EXIT_CODE in
            0)
              if [ "$INFRASTRUCTURE_EXISTS" = "false" ]; then
                echo "‚ö†Ô∏è  No infrastructure exists but plan shows no changes - this may indicate a configuration issue"
                echo ""
                echo "üîç Debugging terraform state and configuration:"
                echo "State file status:"
                terraform state list || echo "No state found"
                echo ""
                echo "üìã Full plan output:"
                cat plan_output.txt
                echo ""
                echo "üîß Terraform configuration check:"
                find . -name "*.tf" -exec echo "File: {}" \; -exec head -5 {} \;
                echo ""
                echo "üîÑ Forcing apply since no infrastructure exists"
                echo "needs_apply=true" >> $GITHUB_OUTPUT
              else
                echo "‚úÖ No changes needed - infrastructure is up to date"
                echo "needs_apply=false" >> $GITHUB_OUTPUT
              fi
              ;;
            1)
              echo "‚ùå Terraform plan failed"
              cat plan_output.txt
              exit 1
              ;;
            2)
              echo "üìù Changes detected - terraform apply needed"
              echo "needs_apply=true" >> $GITHUB_OUTPUT
              
              # Count resources to be created vs modified/destroyed
              RESOURCES_TO_CREATE=$(grep -c "will be created" plan_output.txt || echo "0")
              RESOURCES_TO_REPLACE=$(grep -c "must be replaced" plan_output.txt || echo "0")
              RESOURCES_TO_DESTROY=$(grep -c "will be destroyed" plan_output.txt || echo "0")
              
              echo "  üìä Plan summary:"
              echo "    - Resources to create: $RESOURCES_TO_CREATE"
              echo "    - Resources to replace: $RESOURCES_TO_REPLACE"
              echo "    - Resources to destroy: $RESOURCES_TO_DESTROY"
              
              # Check if plan shows instance replacements
              if [ "$RESOURCES_TO_REPLACE" -gt "0" ]; then
                echo "‚ö†Ô∏è  Plan shows instance replacements - this indicates tainted or incompatible instances"
                echo "tainted=true" >> $GITHUB_OUTPUT
              fi
              ;;
          esac

      - name: Force destroy all instances if any are tainted
        if: github.event.inputs.destroy != 'true' && steps.taint_check.outputs.tainted == 'true'
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: |
          echo "üóëÔ∏è  Tainted instances detected - forcing complete cluster rebuild for consistency"
          echo "‚ö†Ô∏è   This ensures all instances have properly functioning cloud-init and k0s setup"
          echo ""
          
          # Clean up Tailscale entries first
          echo "üßπ Cleaning up Tailscale entries before rebuild..."
          
          # Get all devices from Tailscale API
          DEVICE_RESPONSE=$(curl -s -H "Authorization: Bearer ${{ secrets.TAILSCALE_API_KEY }}" \
            "https://api.tailscale.com/api/v2/tailnet/-/devices" 2>/dev/null)

          if [ -n "$DEVICE_RESPONSE" ] && [ "$DEVICE_RESPONSE" != "null" ]; then
            # Extract all devices that match our patterns
            DEVICES_TO_DELETE=$(echo "$DEVICE_RESPONSE" | jq -r '
              if .devices then 
                .devices[] | select(
                  (.hostname | test("k0s-controller-staging")) or
                  (.hostname | test("k0s-worker-[0-9]+-staging")) or
                  (.hostname | test("^k0s-controller-${{ steps.env.outputs.environment }}")) or
                  (.hostname | test("^k0s-worker-[0-9]+-${{ steps.env.outputs.environment }}"))
                ) | {hostname: .hostname, nodeId: .nodeId}
              else 
                empty 
              end' 2>/dev/null)

            if [ -n "$DEVICES_TO_DELETE" ]; then
              # Parse devices into arrays
              DEVICE_HOSTNAMES=($(echo "$DEVICES_TO_DELETE" | jq -r '.hostname' 2>/dev/null))
              DEVICE_NODEIDS=($(echo "$DEVICES_TO_DELETE" | jq -r '.nodeId' 2>/dev/null))

              echo "Found ${#DEVICE_HOSTNAMES[@]} Tailscale devices to remove before rebuild"
              
              for i in "${!DEVICE_HOSTNAMES[@]}"; do
                hostname="${DEVICE_HOSTNAMES[$i]}"
                nodeId="${DEVICE_NODEIDS[$i]}"
                
                if [ -n "$nodeId" ] && [ "$nodeId" != "null" ]; then
                  curl -s -X DELETE \
                    -H "Authorization: Bearer ${{ secrets.TAILSCALE_API_KEY }}" \
                    "https://api.tailscale.com/api/v2/device/$nodeId" >/dev/null
                  echo "  ‚úÖ Removed $hostname"
                fi
              done
            fi
          fi
          
          echo ""
          echo "üóëÔ∏è  Destroying all instances for clean rebuild..."
          terraform destroy -auto-approve
          
          echo "‚úÖ Clean destroy completed - ready for fresh apply"

      - name: Terraform Apply
        if: github.event.inputs.destroy != 'true' && (steps.taint_check.outputs.needs_apply == 'true' || steps.taint_check.outputs.tainted == 'true')
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        id: apply
        run: |
          echo "üöÄ Applying terraform configuration..."
          terraform apply -auto-approve
          terraform output -json > outputs.json
          
          echo "‚úÖ Infrastructure deployment completed"

      - name: Skip apply - no changes needed
        if: github.event.inputs.destroy != 'true' && steps.taint_check.outputs.needs_apply == 'false' && steps.taint_check.outputs.tainted == 'false'
        run: |
          echo "‚ÑπÔ∏è  Infrastructure is already up to date - skipping apply"
          echo "‚úÖ No changes needed"

      - name: Clean up Tailscale entries before destroy
        if: github.event.inputs.destroy == 'true'
        run: |
          echo "üßπ Cleaning up Tailscale entries for environment: ${{ steps.env.outputs.environment }}"
          
          # Get all devices from Tailscale API
          echo "Fetching all Tailscale devices..."
          DEVICE_RESPONSE=$(curl -s -H "Authorization: Bearer ${{ secrets.TAILSCALE_API_KEY }}" \
            "https://api.tailscale.com/api/v2/tailnet/-/devices" 2>/dev/null)

          # Check if we got a valid response
          if [ -z "$DEVICE_RESPONSE" ] || [ "$DEVICE_RESPONSE" = "null" ]; then
            echo "‚ö†Ô∏è  Could not fetch Tailscale devices (API error or no devices)"
            exit 0
          fi

          # Extract all devices that match our patterns (including partial matches)
          DEVICES_TO_DELETE=$(echo "$DEVICE_RESPONSE" | jq -r '
            if .devices then 
              .devices[] | select(
                (.hostname | test("k0s-controller-staging")) or
                (.hostname | test("k0s-worker-[0-9]+-staging")) or
                (.hostname | test("^k0s-controller-${{ steps.env.outputs.environment }}")) or
                (.hostname | test("^k0s-worker-[0-9]+-${{ steps.env.outputs.environment }}"))
              ) | {hostname: .hostname, nodeId: .nodeId}
            else 
              empty 
            end' 2>/dev/null)

          if [ -z "$DEVICES_TO_DELETE" ]; then
            echo "‚ÑπÔ∏è  No matching Tailscale devices found to clean up"
            exit 0
          fi

          # Parse devices into arrays
          DEVICE_HOSTNAMES=($(echo "$DEVICES_TO_DELETE" | jq -r '.hostname' 2>/dev/null))
          DEVICE_NODEIDS=($(echo "$DEVICES_TO_DELETE" | jq -r '.nodeId' 2>/dev/null))

          echo "Found ${#DEVICE_HOSTNAMES[@]} devices to remove:"
          for i in "${!DEVICE_HOSTNAMES[@]}"; do
            echo "  - ${DEVICE_HOSTNAMES[$i]} (nodeId: ${DEVICE_NODEIDS[$i]})"
          done
          echo ""
          
          # Remove each device with enhanced error handling
          REMOVAL_COUNT=0
          FAILED_REMOVALS=()
          
          for i in "${!DEVICE_HOSTNAMES[@]}"; do
            hostname="${DEVICE_HOSTNAMES[$i]}"
            nodeId="${DEVICE_NODEIDS[$i]}"
            
            echo "Removing $hostname from Tailscale..."

            if [ -n "$nodeId" ] && [ "$nodeId" != "null" ] && [ "$nodeId" != "" ]; then
              echo "  Using nodeId: $nodeId"
              
              # Validate nodeId format (should be alphanumeric)
              if [[ ! "$nodeId" =~ ^[a-zA-Z0-9]+$ ]]; then
                echo "  ‚ùå Invalid nodeId format: $nodeId"
                FAILED_REMOVALS+=("$hostname (invalid nodeId)")
                continue
              fi
              
              # Make delete request with better error handling
              DELETE_RESPONSE=$(curl -s -w "\n%{http_code}" -X DELETE \
                -H "Authorization: Bearer ${{ secrets.TAILSCALE_API_KEY }}" \
                "https://api.tailscale.com/api/v2/device/$nodeId" 2>&1)
              curl_exit_code=$?
              
              if [ $curl_exit_code -ne 0 ]; then
                echo "  ‚ùå curl command failed for $hostname (exit code: $curl_exit_code)"
                echo "  curl error: $DELETE_RESPONSE"
                FAILED_REMOVALS+=("$hostname (curl error)")
                continue
              fi
              
              # Parse response and status code
              http_code=$(echo "$DELETE_RESPONSE" | tail -n1)
              response_body=$(echo "$DELETE_RESPONSE" | head -n -1)
              
              echo "  HTTP Status: $http_code"
              if [ -n "$response_body" ]; then
                echo "  Response: $response_body"
              fi
              
              if [[ "$http_code" == "200" ]] || [[ "$http_code" == "204" ]]; then
                echo "  ‚úÖ Successfully removed $hostname"
                REMOVAL_COUNT=$((REMOVAL_COUNT + 1))
              else
                echo "  ‚ö†Ô∏è  Failed to remove $hostname (HTTP $http_code)"
                FAILED_REMOVALS+=("$hostname (HTTP $http_code)")
              fi
            else
              echo "  ‚ùå No valid nodeId found for $hostname"
              FAILED_REMOVALS+=("$hostname (no nodeId)")
            fi
          done

          echo ""
          echo "üéâ Tailscale cleanup summary:"
          echo "  Total devices: ${#DEVICE_HOSTNAMES[@]}"
          echo "  Successfully removed: $REMOVAL_COUNT"
          echo "  Failed removals: ${#FAILED_REMOVALS[@]}"
          
          if [ ${#FAILED_REMOVALS[@]} -gt 0 ]; then
            echo "  Failed devices:"
            printf "    - %s\n" "${FAILED_REMOVALS[@]}"
            
            # Exit with error if critical failures occurred
            if [ ${#FAILED_REMOVALS[@]} -eq ${#DEVICE_HOSTNAMES[@]} ]; then
              echo "‚ùå All device removals failed - this may indicate an API or authentication issue"
              exit 1
            else
              echo "‚ö†Ô∏è  Some devices failed to remove but continuing with terraform destroy"
            fi
          fi

      - name: Terraform Destroy
        if: github.event.inputs.destroy == 'true'
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: terraform destroy -auto-approve

      - name: Parse Terraform Outputs
        if: github.event.inputs.destroy != 'true' && (steps.taint_check.outputs.needs_apply == 'true' || steps.taint_check.outputs.tainted == 'true')
        id: outputs
        working-directory: terraform/environments/${{ steps.env.outputs.environment }}
        run: |
          if [ -f outputs.json ]; then
            CONTROLLER_IP=$(jq -r '.cluster_info.value.controller.private_ip' outputs.json)
            CONTROLLER_HOSTNAME=$(jq -r '.cluster_info.value.controller.hostname' outputs.json)
            
            echo "controller_ip=$CONTROLLER_IP" >> $GITHUB_OUTPUT
            echo "controller_hostname=$CONTROLLER_HOSTNAME" >> $GITHUB_OUTPUT
            echo "environment=${{ steps.env.outputs.environment }}" >> $GITHUB_OUTPUT
            
            echo "‚úÖ Parsed terraform outputs:"
            echo "  Controller IP: $CONTROLLER_IP"
            echo "  Controller Hostname: $CONTROLLER_HOSTNAME"
            echo "  Environment: ${{ steps.env.outputs.environment }}"
          else
            echo "‚ùå outputs.json not found - this shouldn't happen after apply"
            exit 1
          fi

      - name: Terraform Apply Summary
        if: github.event.inputs.destroy != 'true'
        run: |
          echo "============================================="
          
          if [[ "${{ steps.taint_check.outputs.needs_apply }}" == "true" || "${{ steps.taint_check.outputs.tainted }}" == "true" ]]; then
            if [[ "${{ steps.taint_check.outputs.tainted }}" == "true" ]]; then
              echo "üîÑ Terraform Complete (Clean Rebuild) - ${{ steps.env.outputs.environment }}"
              echo "============================================="
              echo ""
              echo "‚ö†Ô∏è  Tainted instances were detected and rebuilt"
              echo "‚úÖ All instances now have clean cloud-init setup"
            else
              echo "üéâ Terraform Apply Complete - ${{ steps.env.outputs.environment }}"
              echo "============================================="
              echo ""
              echo "‚úÖ Infrastructure changes applied successfully"
            fi
            echo ""
            echo "Controller: ${{ steps.outputs.outputs.controller_hostname }}"
            echo "Private IP: ${{ steps.outputs.outputs.controller_ip }}"
            echo ""
            echo "Next: K8s cluster setup and application deployment will run in separate job"
          else
            echo "‚ÑπÔ∏è  Infrastructure Status Check Complete - ${{ steps.env.outputs.environment }}"
            echo "============================================="
            echo ""
            echo "‚úÖ Infrastructure is already up to date"
            echo ""
            echo "Next: Infrastructure is ready for k8s deployment if needed"
          fi
          echo "============================================="

  # Call the K8s deployment workflow only if infrastructure was applied/changed
  k0s-deployment:
    if: github.event.inputs.destroy != 'true' && needs.terraform-apply.outputs.controller_ip != ''
    needs: terraform-apply
    uses: ./.github/workflows/k0s-deploy.yml
    with:
      environment: ${{ needs.terraform-apply.outputs.environment }}
      controller_ip: ${{ needs.terraform-apply.outputs.controller_ip }}
      controller_hostname: ${{ needs.terraform-apply.outputs.controller_hostname }}
    secrets: inherit
